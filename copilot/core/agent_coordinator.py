"""
Agentåè°ƒå™¨ - ç®¡ç†æ€è€ƒAgentå’Œæ‰§è¡ŒAgentçš„åä½œ
å®ç°"æ€è€ƒ-æ‰§è¡Œ"åŒAgentå·¥ä½œæµ
"""

import json
from datetime import datetime
from typing import Any, AsyncGenerator, Dict, List, Optional

from copilot.core.execution_agent import ExecutionAgent
from copilot.core.thinking_agent import ThinkingAgent, ThinkingResult, ThinkingStep
from copilot.utils.logger import logger


class AgentCoordinator:
    """Agentåè°ƒå™¨ - ç®¡ç†æ€è€ƒå’Œæ‰§è¡ŒAgentçš„åä½œ"""

    def __init__(
        self, thinking_agent: ThinkingAgent, execution_agent: ExecutionAgent, enable_thinking_mode: bool = True, save_thinking_process: bool = True
    ):
        """
        åˆå§‹åŒ–åè°ƒå™¨

        Args:
            thinking_agent: æ€è€ƒAgentå®ä¾‹
            execution_agent: æ‰§è¡ŒAgentå®ä¾‹
            enable_thinking_mode: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            save_thinking_process: æ˜¯å¦ä¿å­˜æ€è€ƒè¿‡ç¨‹
        """
        self.thinking_agent = thinking_agent
        self.execution_agent = execution_agent
        self.enable_thinking_mode = enable_thinking_mode
        self.save_thinking_process = save_thinking_process

        # æ€è€ƒè¿‡ç¨‹å­˜å‚¨
        self.thinking_history: Dict[str, List[ThinkingResult]] = {}

        logger.info(f"AgentCoordinator initialized with thinking_mode={enable_thinking_mode}")

    @classmethod
    async def create_with_mcp_tools(
        cls,
        thinking_provider: str = "deepseek",
        thinking_model: str = "deepseek-chat",
        execution_provider: str = "deepseek",
        execution_model: str = "deepseek-chat",
        enable_thinking_mode: bool = True,
        save_thinking_process: bool = True,
        **llm_kwargs,
    ):
        """
        å¼‚æ­¥åˆ›å»ºAgentCoordinatorå®ä¾‹ï¼Œè‡ªåŠ¨åŠ è½½MCPå·¥å…·

        Args:
            thinking_provider: æ€è€ƒAgentçš„LLMæä¾›å•†
            thinking_model: æ€è€ƒAgentçš„æ¨¡å‹åç§°
            execution_provider: æ‰§è¡ŒAgentçš„LLMæä¾›å•†
            execution_model: æ‰§è¡ŒAgentçš„æ¨¡å‹åç§°
            enable_thinking_mode: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            save_thinking_process: æ˜¯å¦ä¿å­˜æ€è€ƒè¿‡ç¨‹
            **llm_kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°

        Returns:
            AgentCoordinator: é…ç½®å¥½çš„åè°ƒå™¨å®ä¾‹
        """
        # åˆ›å»ºå¸¦æœ‰MCPå·¥å…·çš„æ€è€ƒAgent
        thinking_agent = await ThinkingAgent.create_with_mcp_tools(
            provider=thinking_provider,
            model_name=thinking_model,
            **llm_kwargs,
        )

        # åˆ›å»ºå¸¦æœ‰MCPå·¥å…·çš„æ‰§è¡ŒAgent
        execution_agent = await ExecutionAgent.create_with_mcp_tools(
            provider=execution_provider,
            model_name=execution_model,
            **llm_kwargs,
        )

        logger.info(f"Creating AgentCoordinator with thinking_mode={enable_thinking_mode}")

        # åˆ›å»ºåè°ƒå™¨å®ä¾‹
        return cls(
            thinking_agent=thinking_agent,
            execution_agent=execution_agent,
            enable_thinking_mode=enable_thinking_mode,
            save_thinking_process=save_thinking_process,
        )

    async def process_user_input(
        self,
        user_input: str,
        session_id: str,
        thread_id: Optional[str] = None,
        images: Optional[List] = None,
        enable_tools: bool = True,
        context: Dict[str, Any] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œåè°ƒæ€è€ƒå’Œæ‰§è¡Œè¿‡ç¨‹

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            session_id: ä¼šè¯ID
            thread_id: çº¿ç¨‹ID
            images: å›¾ç‰‡åˆ—è¡¨
            enable_tools: æ˜¯å¦å¯ç”¨å·¥å…·
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Yields:
            Dict[str, Any]: æµå¼å“åº”æ•°æ®
        """
        try:
            # å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œå…ˆè¿›è¡Œæ€è€ƒ
            if self.enable_thinking_mode:
                async for chunk in self._process_with_thinking(user_input, session_id, thread_id, images, enable_tools, context):
                    yield chunk
            else:
                # ç›´æ¥æ‰§è¡Œæ¨¡å¼
                async for chunk in self._process_direct_execution(user_input, session_id, thread_id, images, enable_tools):
                    yield chunk

        except Exception as e:
            logger.error(f"AgentCoordinatorå¤„ç†å‡ºé”™: {str(e)}")
            yield {"type": "error", "content": f"åè°ƒå™¨å¤„ç†å‡ºé”™: {str(e)}", "timestamp": datetime.now().isoformat()}

    async def _process_with_thinking(
        self, user_input: str, session_id: str, thread_id: Optional[str], images: Optional[List], enable_tools: bool, context: Dict[str, Any]
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """å¸¦æ€è€ƒçš„å¤„ç†æµç¨‹"""

        # 1. å¼€å§‹æ€è€ƒé˜¶æ®µ
        yield {"type": "thinking_start", "content": "ğŸ¤” æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...", "phase": "thinking", "timestamp": datetime.now().isoformat()}

        try:
            # 2. è·å–å¯¹è¯å†å²ç”¨äºæ€è€ƒ
            conversation_history = await self._get_conversation_history(session_id)

            thinking_result = None

            # 3. æµå¼æ€è€ƒAgentåˆ†æ
            async for thinking_chunk in self.thinking_agent.think_stream(
                user_input=user_input, context=context, conversation_history=conversation_history
            ):
                # ç›´æ¥è½¬å‘æ€è€ƒæµæ•°æ®
                yield thinking_chunk

                # å¦‚æœæ˜¯æ€è€ƒå®Œæˆï¼Œä¿å­˜ç»“æœç”¨äºåç»­æ‰§è¡Œ
                if thinking_chunk.get("type") == "thinking_complete" and "thinking_data" in thinking_chunk:
                    # ä»thinking_dataé‡æ„ThinkingResultå¯¹è±¡
                    thinking_data = thinking_chunk["thinking_data"]

                    # é‡æ„ThinkingStepå¯¹è±¡
                    execution_plan = []
                    for step_data in thinking_data.get("execution_plan", []):
                        step = ThinkingStep(
                            step_id=step_data.get("step_id", f"step_{len(execution_plan)+1}"),
                            description=step_data.get("description", ""),
                            reasoning=step_data.get("reasoning", ""),
                            expected_tools=step_data.get("expected_tools", []),
                            parameters=step_data.get("parameters", {}),
                            priority=step_data.get("priority", 1),
                            dependencies=step_data.get("dependencies", []),
                        )
                        execution_plan.append(step)

                    # é‡æ„ThinkingResultå¯¹è±¡
                    thinking_result = ThinkingResult(
                        user_intent=thinking_data.get("user_intent", ""),
                        problem_analysis=thinking_data.get("problem_analysis", ""),
                        execution_plan=execution_plan,
                        estimated_complexity=thinking_data.get("estimated_complexity", "medium"),
                        suggested_model=thinking_data.get("suggested_model"),
                        context_requirements=thinking_data.get("context_requirements", {}),
                        timestamp=datetime.now(),
                    )

            # 4. ä¿å­˜æ€è€ƒè¿‡ç¨‹
            if thinking_result and self.save_thinking_process:
                self._save_thinking_result(session_id, thinking_result)

            # 5. æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å¾—æ€è€ƒç»“æœ
            if not thinking_result:
                # å¦‚æœæ²¡æœ‰è·å¾—æ€è€ƒç»“æœï¼Œåˆ›å»ºå¤‡ç”¨ç»“æœ
                logger.warning("æœªèƒ½è·å¾—æœ‰æ•ˆçš„æ€è€ƒç»“æœï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                thinking_result = self.thinking_agent._create_fallback_result(user_input)

                if self.save_thinking_process:
                    self._save_thinking_result(session_id, thinking_result)

            # 6. å¼€å§‹æ‰§è¡Œé˜¶æ®µ
            yield {
                "type": "execution_start",
                "content": "âš¡ å¼€å§‹æ‰§è¡Œä»»åŠ¡...",
                "phase": "execution",
                "execution_plan": [
                    {
                        "step_id": step.step_id,
                        "description": step.description,
                        "reasoning": step.reasoning,
                        "expected_tools": step.expected_tools or [],
                        "parameters": step.parameters or {},
                        "priority": step.priority,
                        "dependencies": step.dependencies or [],
                    }
                    for step in thinking_result.execution_plan
                ],
                "timestamp": datetime.now().isoformat(),
            }

            # 7. æ„å»ºå¢å¼ºçš„æ‰§è¡Œè¾“å…¥ï¼ŒåŒ…å«å·¥å…·å»ºè®®
            enhanced_input = self._build_enhanced_input(user_input, thinking_result)
            
            # è·å–æ€è€ƒç»“æœä¸­å»ºè®®çš„å·¥å…·
            suggested_tools = []
            if hasattr(thinking_result, 'execution_plan') and thinking_result.execution_plan:
                for step in thinking_result.execution_plan:
                    if step.expected_tools:
                        suggested_tools.extend(step.expected_tools)
            
            # å¦‚æœæœ‰å»ºè®®çš„å·¥å…·ï¼Œåœ¨è¾“å…¥ä¸­æ˜ç¡®æåŠ
            if suggested_tools:
                enhanced_input += f"\n\nğŸ’¡ **å»ºè®®ä½¿ç”¨çš„å·¥å…·**: {', '.join(suggested_tools)}"
                enhanced_input += "\nè¯·ä¼˜å…ˆè€ƒè™‘ä½¿ç”¨è¿™äº›å·¥å…·æ¥å®Œæˆåˆ†æä»»åŠ¡ã€‚"

            # 8. æ‰§è¡ŒAgentå¤„ç†
            async for chunk in self.execution_agent.chat(
                message=enhanced_input, thread_id=thread_id, session_id=session_id, images=images, enable_tools=enable_tools
            ):
                # ä¸ºæ‰§è¡Œé˜¶æ®µçš„è¾“å‡ºæ·»åŠ æ ‡è¯†
                if isinstance(chunk, dict):
                    chunk["phase"] = "execution"
                    yield chunk
                else:
                    yield {"type": "content", "content": chunk, "phase": "execution", "timestamp": datetime.now().isoformat()}

            # 9. æ‰§è¡Œå®Œæˆ
            yield {"type": "execution_complete", "content": "âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ", "phase": "complete", "timestamp": datetime.now().isoformat()}

        except Exception as e:
            logger.error(f"æ€è€ƒ-æ‰§è¡Œæµç¨‹å‡ºé”™: {str(e)}")
            yield {"type": "error", "content": f"æ€è€ƒæˆ–æ‰§è¡Œè¿‡ç¨‹å‡ºé”™: {str(e)}", "phase": "error", "timestamp": datetime.now().isoformat()}

    async def _process_direct_execution(
        self, user_input: str, session_id: str, thread_id: Optional[str], images: Optional[List], enable_tools: bool
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ç›´æ¥æ‰§è¡Œæ¨¡å¼"""

        yield {"type": "execution_start", "content": "ç›´æ¥å¤„ç†æ‚¨çš„è¯·æ±‚...", "phase": "execution", "timestamp": datetime.now().isoformat()}

        async for chunk in self.execution_agent.chat(
            message=user_input, thread_id=thread_id, session_id=session_id, images=images, enable_tools=enable_tools
        ):
            if isinstance(chunk, dict):
                chunk["phase"] = "execution"
                yield chunk
            else:
                yield {"type": "content", "content": chunk, "phase": "execution", "timestamp": datetime.now().isoformat()}

    def _format_thinking_result(self, result: ThinkingResult) -> str:
        """æ ¼å¼åŒ–æ€è€ƒç»“æœä¸ºç”¨æˆ·å¯è¯»çš„æ–‡æœ¬"""
        formatted_text = f"""ğŸ’­ **åˆ†æç»“æœ**

**ç”¨æˆ·æ„å›¾**: {result.user_intent}

**é—®é¢˜åˆ†æ**: {result.problem_analysis}

**æ‰§è¡Œè®¡åˆ’**:
"""

        for i, step in enumerate(result.execution_plan, 1):
            formatted_text += f"\n{i}. **{step.description}**"
            formatted_text += f"\n   - åŸå› : {step.reasoning}"
            if step.expected_tools:
                formatted_text += f"\n   - é¢„æœŸå·¥å…·: {', '.join(step.expected_tools)}"
            if step.dependencies:
                formatted_text += f"\n   - ä¾èµ–: {', '.join(step.dependencies)}"

        formatted_text += f"\n\n**å¤æ‚åº¦è¯„ä¼°**: {result.estimated_complexity}"

        if result.suggested_model:
            formatted_text += f"\n**å»ºè®®æ¨¡å‹**: {result.suggested_model}"

        return formatted_text

    def _build_enhanced_input(self, original_input: str, thinking_result: ThinkingResult) -> str:
        """æ„å»ºå¢å¼ºçš„æ‰§è¡Œè¾“å…¥ï¼ŒåŒ…å«æ€è€ƒç»“æœ"""

        enhanced_input = f"""ç”¨æˆ·åŸå§‹è¾“å…¥: {original_input}

åŸºäºæ·±åº¦åˆ†æï¼Œæˆ‘éœ€è¦æŒ‰ä»¥ä¸‹è®¡åˆ’æ‰§è¡Œ:

ç”¨æˆ·æ„å›¾: {thinking_result.user_intent}
é—®é¢˜åˆ†æ: {thinking_result.problem_analysis}

æ‰§è¡Œæ­¥éª¤:
"""

        for i, step in enumerate(thinking_result.execution_plan, 1):
            enhanced_input += f"\n{i}. {step.description}"
            enhanced_input += f"\n   åŸå› : {step.reasoning}"
            if step.expected_tools:
                enhanced_input += f"\n   éœ€è¦å·¥å…·: {', '.join(step.expected_tools)}"
            if step.parameters:
                enhanced_input += f"\n   å‚æ•°: {json.dumps(step.parameters, ensure_ascii=False)}"

        enhanced_input += "\n\nè¯·æŒ‰ç…§è¿™ä¸ªè®¡åˆ’æ‰§è¡Œï¼Œç¡®ä¿æ¯ä¸ªæ­¥éª¤éƒ½å¾—åˆ°é€‚å½“çš„å¤„ç†ã€‚"

        return enhanced_input

    async def _get_conversation_history(self, session_id: str) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        try:
            # ä»æ‰§è¡ŒAgentçš„å†å²ç®¡ç†å™¨è·å–å†å²è®°å½•
            if hasattr(self.execution_agent, "chat_history_manager"):
                history_messages = await self.execution_agent.chat_history_manager.get_session_messages(session_id=session_id, limit=10)

                # è½¬æ¢ä¸ºç®€å•çš„å­—å…¸æ ¼å¼
                conversation_history = []
                for msg in history_messages:
                    conversation_history.append(
                        {"role": msg.role, "content": msg.content, "timestamp": msg.timestamp.isoformat() if msg.timestamp else None}
                    )

                return conversation_history

        except Exception as e:
            logger.warning(f"è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")

        return []

    def _save_thinking_result(self, session_id: str, result: ThinkingResult):
        """ä¿å­˜æ€è€ƒç»“æœ"""
        if session_id not in self.thinking_history:
            self.thinking_history[session_id] = []

        self.thinking_history[session_id].append(result)

        # åªä¿ç•™æœ€è¿‘20æ¬¡æ€è€ƒè®°å½•
        if len(self.thinking_history[session_id]) > 20:
            self.thinking_history[session_id] = self.thinking_history[session_id][-20:]

    def get_thinking_history(self, session_id: str) -> List[ThinkingResult]:
        """è·å–ä¼šè¯çš„æ€è€ƒå†å²"""
        return self.thinking_history.get(session_id, [])

    def clear_thinking_history(self, session_id: str):
        """æ¸…é™¤ä¼šè¯çš„æ€è€ƒå†å²"""
        if session_id in self.thinking_history:
            del self.thinking_history[session_id]

    def configure_thinking_mode(self, enabled: bool):
        """é…ç½®æ€è€ƒæ¨¡å¼"""
        self.enable_thinking_mode = enabled
        logger.info(f"Thinking mode set to: {enabled}")

    def get_coordinator_stats(self) -> Dict[str, Any]:
        """è·å–åè°ƒå™¨ç»Ÿè®¡ä¿¡æ¯"""
        total_thinking_sessions = len(self.thinking_history)
        total_thinking_records = sum(len(history) for history in self.thinking_history.values())

        return {
            "thinking_mode_enabled": self.enable_thinking_mode,
            "save_thinking_process": self.save_thinking_process,
            "total_thinking_sessions": total_thinking_sessions,
            "total_thinking_records": total_thinking_records,
            "thinking_agent_stats": self.thinking_agent.get_thinking_stats(),
            "execution_agent_info": {
                "provider": self.execution_agent.provider,
                "model": self.execution_agent.model_name,
                "context_memory_enabled": getattr(self.execution_agent, "context_memory_enabled", False),
            },
        }

    async def refine_and_retry(
        self,
        session_id: str,
        feedback: str,
        original_input: str,
        thread_id: Optional[str] = None,
        images: Optional[List] = None,
        enable_tools: bool = True,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æ ¹æ®åé¦ˆä¼˜åŒ–è®¡åˆ’å¹¶é‡è¯•æ‰§è¡Œ

        Args:
            session_id: ä¼šè¯ID
            feedback: ç”¨æˆ·åé¦ˆæˆ–é”™è¯¯ä¿¡æ¯
            original_input: åŸå§‹ç”¨æˆ·è¾“å…¥
            thread_id: çº¿ç¨‹ID
            images: å›¾ç‰‡åˆ—è¡¨
            enable_tools: æ˜¯å¦å¯ç”¨å·¥å…·

        Yields:
            Dict[str, Any]: æµå¼å“åº”æ•°æ®
        """
        try:
            # è·å–æœ€è¿‘çš„æ€è€ƒç»“æœ
            thinking_history = self.get_thinking_history(session_id)
            if not thinking_history:
                yield {"type": "error", "content": "æ²¡æœ‰æ‰¾åˆ°ä¹‹å‰çš„æ€è€ƒè®°å½•ï¼Œæ— æ³•ä¼˜åŒ–", "timestamp": datetime.now().isoformat()}
                return

            last_thinking = thinking_history[-1]

            # ä¼˜åŒ–è®¡åˆ’
            yield {
                "type": "refining_start",
                "content": "ğŸ”„ æ­£åœ¨æ ¹æ®åé¦ˆä¼˜åŒ–æ‰§è¡Œè®¡åˆ’...",
                "phase": "refining",
                "timestamp": datetime.now().isoformat(),
            }

            # ä½¿ç”¨åŸæœ‰çš„refine_planæ–¹æ³•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            refined_result = await self.thinking_agent.refine_plan(last_thinking, feedback)

            # ä¿å­˜ä¼˜åŒ–åçš„è®¡åˆ’
            if self.save_thinking_process:
                self._save_thinking_result(session_id, refined_result)

            # è¾“å‡ºä¼˜åŒ–ç»“æœ
            yield {
                "type": "refined_plan",
                "content": f"ğŸ“‹ ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’:\n{self._format_thinking_result(refined_result)}",
                "thinking_data": {
                    "user_intent": refined_result.user_intent,
                    "problem_analysis": refined_result.problem_analysis,
                    "execution_plan": [
                        {
                            "step_id": step.step_id,
                            "description": step.description,
                            "reasoning": step.reasoning,
                            "expected_tools": step.expected_tools or [],
                            "parameters": step.parameters or {},
                            "priority": step.priority,
                            "dependencies": step.dependencies or [],
                        }
                        for step in refined_result.execution_plan
                    ],
                    "estimated_complexity": refined_result.estimated_complexity,
                    "suggested_model": refined_result.suggested_model,
                    "context_requirements": refined_result.context_requirements or {},
                    "timestamp": refined_result.timestamp.isoformat() if refined_result.timestamp else datetime.now().isoformat(),
                },
                "phase": "refining",
                "timestamp": datetime.now().isoformat(),
            }

            # é‡æ–°æ‰§è¡Œ
            yield {
                "type": "retry_execution_start",
                "content": "ğŸ”„ å¼€å§‹é‡æ–°æ‰§è¡Œ...",
                "phase": "retry_execution",
                "timestamp": datetime.now().isoformat(),
            }

            enhanced_input = self._build_enhanced_input(original_input, refined_result)

            async for chunk in self.execution_agent.chat(
                message=enhanced_input, thread_id=thread_id, session_id=session_id, images=images, enable_tools=enable_tools
            ):
                if isinstance(chunk, dict):
                    chunk["phase"] = "retry_execution"
                    yield chunk
                else:
                    yield {"type": "content", "content": chunk, "phase": "retry_execution", "timestamp": datetime.now().isoformat()}

        except Exception as e:
            logger.error(f"ä¼˜åŒ–é‡è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}")
            yield {"type": "error", "content": f"ä¼˜åŒ–é‡è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}", "timestamp": datetime.now().isoformat()}
