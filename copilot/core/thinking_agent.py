"""
æ€è€ƒAgent - ä¸“é—¨è´Ÿè´£è§£è¯»ç”¨æˆ·è¾“å…¥ã€åˆ†æé—®é¢˜å’Œåˆ¶å®šå¤„ç†æ­¥éª¤
ä¸æ‰§è¡Œå®é™…å·¥å…·ï¼Œåªè¿›è¡Œæ€è€ƒå’Œè§„åˆ’
"""

import json
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

from copilot.core.llm_factory import LLMFactory
from copilot.utils.logger import logger


@dataclass
class ThinkingStep:
    """æ€è€ƒæ­¥éª¤"""

    step_id: str
    description: str
    reasoning: str
    expected_tools: List[str] = None
    parameters: Dict[str, Any] = None
    priority: int = 1
    dependencies: List[str] = None


@dataclass
class ThinkingResult:
    """æ€è€ƒç»“æœ"""

    user_intent: str
    problem_analysis: str
    execution_plan: List[ThinkingStep]
    estimated_complexity: str  # low, medium, high
    suggested_model: str = None
    context_requirements: Dict[str, Any] = None
    timestamp: datetime = None


class ThinkingAgent:
    """æ€è€ƒAgent - ä¸“é—¨è´Ÿè´£åˆ†æå’Œè§„åˆ’ï¼Œä¸æ‰§è¡Œå®é™…æ“ä½œ"""

    def __init__(self, provider: str = "deepseek", model_name: str = "deepseek-chat", mcp_tools: List = None, **llm_kwargs):
        """
        åˆå§‹åŒ–æ€è€ƒAgent

        Args:
            provider: LLMæä¾›å•†ï¼ˆå»ºè®®ä½¿ç”¨æ¨ç†èƒ½åŠ›å¼ºçš„æ¨¡å‹ï¼‰
            model_name: æ¨¡å‹åç§°
            mcp_tools: MCPå·¥å…·åˆ—è¡¨ï¼ˆç”¨äºåˆ†æå¯ç”¨å·¥å…·ï¼‰
            **llm_kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°
        """
        self.provider = provider
        self.model_name = model_name
        self.mcp_tools = mcp_tools or []
        self.llm_kwargs = llm_kwargs

        # åˆå§‹åŒ–LLMï¼ˆä½¿ç”¨æ›´é«˜çš„temperatureä»¥è·å¾—æ›´å¤šåˆ›é€ æ€§æ€è€ƒï¼‰
        llm_config = {"temperature": 0.7, "max_tokens": 4000, **llm_kwargs}  # ç¨å¾®æé«˜åˆ›é€ æ€§  # ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´è¿›è¡Œè¯¦ç»†æ€è€ƒ

        self.llm = LLMFactory.create_llm(provider=provider, model=model_name, **llm_config)

        # æ€è€ƒpromptæ¨¡æ¿
        self.thinking_prompt = self._build_thinking_prompt()

        logger.info(f"ThinkingAgent initialized with {provider}/{model_name}, {len(self.mcp_tools)} MCP tools available")

    @classmethod
    async def create_with_mcp_tools(
        cls,
        provider: str = "deepseek",
        model_name: str = "deepseek-chat",
        **llm_kwargs,
    ):
        """
        å¼‚æ­¥åˆ›å»ºæ€è€ƒAgentå®ä¾‹ï¼Œè‡ªåŠ¨åŠ è½½MCPå·¥å…·

        Args:
            provider: LLMæä¾›å•†
            model_name: æ¨¡å‹åç§°
            **llm_kwargs: ä¼ é€’ç»™LLMçš„é¢å¤–å‚æ•°

        Returns:
            ThinkingAgent: é…ç½®å¥½çš„æ€è€ƒAgentå®ä¾‹
        """
        # è·å–å¯ç”¨çš„MCPå·¥å…·
        from copilot.core.mcp_tool_wrapper import MCPToolWrapper

        mcp_tools = await MCPToolWrapper.get_mcp_tools()

        logger.info(f"Creating ThinkingAgent with provider: {provider}, model: {model_name}, mcp_tools: {len(mcp_tools)}")

        # åˆ›å»ºæ€è€ƒAgentå®ä¾‹
        return cls(
            provider=provider,
            model_name=model_name,
            mcp_tools=mcp_tools,
            **llm_kwargs,
        )

    def _build_thinking_prompt(self) -> str:
        """æ„å»ºæ€è€ƒAgentçš„promptæ¨¡æ¿ - åŒ…å«å¯ç”¨å·¥å…·ä¿¡æ¯"""
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ€è€ƒåŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·è¾“å…¥å¹¶åˆ¶å®šå¤„ç†è®¡åˆ’ã€‚

è¯·ä»¥è‡ªç„¶ã€æµç•…çš„æ–¹å¼åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶åˆ¶å®šæ‰§è¡Œè®¡åˆ’ã€‚ä½ å¯ä»¥è¿™æ ·æ€è€ƒï¼š

1. é¦–å…ˆç†è§£ç”¨æˆ·æƒ³è¦ä»€ä¹ˆ
2. åˆ†æè¿™ä¸ªé—®é¢˜çš„å¤æ‚ç¨‹åº¦
3. åˆ¶å®šå…·ä½“çš„æ‰§è¡Œæ­¥éª¤
4. è€ƒè™‘éœ€è¦ä»€ä¹ˆå·¥å…·å’Œèµ„æº

è¯·ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼Œå°±åƒåœ¨å’Œæœ‹å‹è®¨è®ºå¦‚ä½•è§£å†³é—®é¢˜ä¸€æ ·ã€‚

ç°åœ¨å¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥ï¼š"""

        # å¦‚æœæœ‰å¯ç”¨çš„MCPå·¥å…·ï¼Œæ·»åŠ åˆ°promptä¸­
        if self.mcp_tools:
            prompt += "\n\nğŸ“‹ **å¯ç”¨çš„å·¥å…·åˆ—è¡¨**:\n"
            for i, tool in enumerate(self.mcp_tools, 1):
                tool_name = getattr(tool, "name", str(tool))
                tool_desc = getattr(tool, "description", "æ— æè¿°")
                prompt += f"{i}. **{tool_name}**: {tool_desc}\n"

            prompt += "\nğŸ’¡ **æç¤º**: åœ¨åˆ¶å®šæ‰§è¡Œè®¡åˆ’æ—¶ï¼Œè¯·è€ƒè™‘ä½¿ç”¨ä¸Šè¿°å·¥å…·æ¥å®Œæˆç‰¹å®šä»»åŠ¡ã€‚"

        return prompt

    async def think(self, user_input: str, context: Dict[str, Any] = None, conversation_history: List[Dict] = None) -> ThinkingResult:
        """
        åˆ†æç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆæ€è€ƒç»“æœ

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¼šè¯ä¿¡æ¯ã€ç”¨æˆ·åå¥½ç­‰ï¼‰
            conversation_history: å¯¹è¯å†å²

        Returns:
            ThinkingResult: è¯¦ç»†çš„æ€è€ƒå’Œè§„åˆ’ç»“æœ
        """
        try:
            # æ„å»ºå®Œæ•´çš„æ€è€ƒè¾“å…¥
            thinking_input = self._build_thinking_input(user_input, context, conversation_history)

            logger.info(f"ThinkingAgentå¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥: {user_input[:100]}...")

            # è°ƒç”¨LLMè¿›è¡Œæ€è€ƒ
            response = await self.llm.ainvoke(thinking_input)

            # è§£ææ€è€ƒç»“æœ
            result = self._parse_thinking_response(response.content)

            logger.info(f"ThinkingAgentå®Œæˆåˆ†æï¼Œç”Ÿæˆäº†{len(result.execution_plan)}ä¸ªæ‰§è¡Œæ­¥éª¤")

            return result

        except Exception as e:
            logger.error(f"ThinkingAgentæ€è€ƒè¿‡ç¨‹å‡ºé”™: {str(e)}")
            # è¿”å›åŸºç¡€çš„æ€è€ƒç»“æœ
            return self._create_fallback_result(user_input)

    async def think_stream(self, user_input: str, context: Dict[str, Any] = None, conversation_history: List[Dict] = None):
        """
        æµå¼æ€è€ƒæ–¹æ³• - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è¾“å‡ºAIçš„æ€è€ƒè¿‡ç¨‹

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¼šè¯ä¿¡æ¯ã€ç”¨æˆ·åå¥½ç­‰ï¼‰
            conversation_history: å¯¹è¯å†å²

        Yields:
            Dict[str, Any]: æµå¼æ€è€ƒæ•°æ®
        """
        try:
            # æ„å»ºå®Œæ•´çš„æ€è€ƒè¾“å…¥
            thinking_input = self._build_thinking_input(user_input, context, conversation_history)

            logger.info(f"ThinkingAgentå¼€å§‹æµå¼åˆ†æç”¨æˆ·è¾“å…¥: {user_input[:100]}...")

            # ä½¿ç”¨æµå¼è°ƒç”¨ï¼Œç›´æ¥è¾“å‡ºAIçš„æ€è€ƒè¿‡ç¨‹
            full_response = ""
            thinking_buffer = ""

            async for chunk in self.llm.astream(thinking_input):
                if hasattr(chunk, "content") and chunk.content:
                    content = str(chunk.content)
                    full_response += content
                    thinking_buffer += content

                    # ç®€å•çš„åˆ†å—è¾“å‡ºï¼šå½“æœ‰å®Œæ•´å¥å­æˆ–è¶³å¤Ÿé•¿åº¦æ—¶è¾“å‡º
                    if len(thinking_buffer) > 30 and any(char in thinking_buffer for char in "ã€‚ï¼ï¼Ÿ\n"):
                        yield {
                            "type": "thinking_chunk",
                            "content": thinking_buffer.strip(),
                            "phase": "thinking",
                            "timestamp": datetime.now().isoformat(),
                        }
                        thinking_buffer = ""

                    # ç®€å•çš„åˆ†å—è¾“å‡ºï¼šå½“æœ‰å®Œæ•´å¥å­æˆ–è¶³å¤Ÿé•¿åº¦æ—¶è¾“å‡º
                    if len(thinking_buffer) > 30 and any(char in thinking_buffer for char in "ã€‚ï¼ï¼Ÿ\n"):
                        # æ¸…ç†å†…å®¹ï¼Œç§»é™¤markdownæ ‡è®°
                        clean_content = self._clean_thinking_chunk(thinking_buffer.strip())
                        if clean_content:
                            yield {
                                "type": "thinking_chunk",
                                "content": clean_content,
                                "phase": "thinking",
                                "timestamp": datetime.now().isoformat(),
                            }
                        thinking_buffer = ""

            # è¾“å‡ºå‰©ä½™çš„æ€è€ƒå†…å®¹
            if thinking_buffer.strip():
                clean_content = self._clean_thinking_chunk(thinking_buffer.strip())
                if clean_content:
                    yield {"type": "thinking_chunk", "content": clean_content, "phase": "thinking", "timestamp": datetime.now().isoformat()}

            # æ€è€ƒå®Œæˆï¼Œåˆ›å»ºJSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®
            if full_response:
                # åˆ›å»ºç»“æ„åŒ–çš„æ€è€ƒç»“æœ
                thinking_result = self._create_structured_result(full_response, user_input)

                # è§£æJSONå­—ç¬¦ä¸²ä¸ºå­—å…¸ï¼Œç”¨äºåç»­å¤„ç†
                try:
                    thinking_data = json.loads(thinking_result)
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºåŸºç¡€æ•°æ®ç»“æ„
                    thinking_data = {
                        "status": "completed",
                        "user_input": user_input,
                        "user_intent": user_input[:200] + "..." if len(user_input) > 200 else user_input,
                        "problem_analysis": "æ€è€ƒåˆ†æå®Œæˆ",
                        "execution_plan": [
                            {
                                "step_id": "step_1",
                                "description": "åŸºäºæ€è€ƒç»“æœæ‰§è¡Œç”¨æˆ·è¯·æ±‚",
                                "reasoning": "æ ¹æ®AIçš„æ€è€ƒåˆ†ææ‰§è¡Œä»»åŠ¡",
                                "expected_tools": [],
                                "parameters": {},
                                "priority": 1,
                                "dependencies": [],
                            }
                        ],
                        "complexity": "medium",
                        "complexity_level": "medium",
                    }

                yield {
                    "type": "thinking_complete",
                    "content": thinking_result,  # ä¿æŒåŸæœ‰çš„contentå­—æ®µ
                    "thinking_data": thinking_data,  # æ·»åŠ thinking_dataå­—æ®µä¾›coordinatorä½¿ç”¨
                    "phase": "thinking",
                    "timestamp": datetime.now().isoformat(),
                }

            logger.info("ThinkingAgentå®Œæˆæµå¼åˆ†æ")

        except Exception as e:
            logger.error(f"ThinkingAgentæµå¼æ€è€ƒè¿‡ç¨‹å‡ºé”™: {str(e)}")

            yield {
                "type": "thinking_error",
                "content": f"ğŸš« æ€è€ƒè¿‡ç¨‹é‡åˆ°é”™è¯¯: {str(e)}",
                "phase": "thinking",
                "timestamp": datetime.now().isoformat(),
            }

            # åˆ›å»ºç®€å•çš„å¤‡ç”¨ç»“æœ - JSONæ ¼å¼
            fallback_result = {
                "status": "error",
                "user_input": user_input,
                "user_intent": user_input[:200] + "..." if len(user_input) > 200 else user_input,
                "problem_analysis": "æ€è€ƒè¿‡ç¨‹é‡åˆ°é—®é¢˜ï¼Œå°†ç›´æ¥å¤„ç†ç”¨æˆ·è¯·æ±‚",
                "key_points": ["ç³»ç»Ÿå°†ç›´æ¥å¤„ç†ç”¨æˆ·è¯·æ±‚"],
                "execution_plan": [
                    {
                        "step_id": "step_1",
                        "description": f"å¤„ç†ç”¨æˆ·è¯·æ±‚: {user_input[:100]}{'...' if len(user_input) > 100 else ''}",
                        "reasoning": "æ€è€ƒAgentå‡ºé”™ï¼Œç›´æ¥å¤„ç†ç”¨æˆ·è¾“å…¥",
                        "expected_tools": ["web_search"],  # é»˜è®¤å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢
                        "parameters": {},
                        "priority": 1,
                        "dependencies": [],
                    }
                ],
                "estimated_complexity": "medium",
                "complexity_level": "medium",
                "suggested_model": None,
                "context_requirements": {},
                "thinking_duration": "error",
                "timestamp": datetime.now().isoformat(),
                "metadata": {"response_length": 0, "key_points_count": 1, "analysis_quality": "error"},
            }

            yield {
                "type": "thinking_complete",
                "content": json.dumps(fallback_result, ensure_ascii=False, indent=2),
                "thinking_data": fallback_result,  # æ·»åŠ thinking_dataå­—æ®µä¾›coordinatorä½¿ç”¨
                "phase": "thinking",
                "timestamp": datetime.now().isoformat(),
            }

    def _create_simple_summary(self, full_response: str, user_input: str) -> str:
        """åˆ›å»ºç®€å•çš„æ€è€ƒæ€»ç»“ - ä¼˜åŒ–ç‰ˆæœ¬"""

        # æ¸…ç†å’Œæ ¼å¼åŒ–æ€è€ƒå†…å®¹
        cleaned_response = self._clean_summary_content(full_response)

        # æå–å…³é”®ä¿¡æ¯
        summary_parts = []

        # 1. ç”¨æˆ·æ„å›¾ï¼ˆä»ç”¨æˆ·è¾“å…¥æå–ï¼‰
        user_intent = user_input[:100] + "..." if len(user_input) > 100 else user_input
        summary_parts.append(f"ğŸ¯ **ç”¨æˆ·éœ€æ±‚**: {user_intent}")

        # 2. æ€è€ƒè¦ç‚¹ï¼ˆä»AIå“åº”ä¸­æå–ï¼‰
        key_points = self._extract_key_points(cleaned_response)
        if key_points:
            summary_parts.append("ğŸ’­ **æ€è€ƒè¦ç‚¹**")
            for i, point in enumerate(key_points, 1):
                summary_parts.append(f"{i}. {point}")

        # 3. æ‰§è¡Œè®¡åˆ’
        summary_parts.append("ğŸ“‹ **æ‰§è¡Œè®¡åˆ’**")
        summary_parts.append("1. åŸºäºåˆ†æç»“æœå¤„ç†ç”¨æˆ·è¯·æ±‚")

        # 4. å¤æ‚åº¦è¯„ä¼°
        complexity = self._assess_complexity(cleaned_response)
        summary_parts.append(f"\n{complexity}")

        return "\n\n".join(summary_parts)

    def _clean_summary_content(self, content: str) -> str:
        """æ¸…ç†æ€»ç»“å†…å®¹ï¼Œç§»é™¤markdownæ ‡è®°ç­‰"""
        if not content:
            return ""

        # ç§»é™¤markdownæ ‡è®°
        content = content.replace("###", "").replace("##", "").replace("#", "")
        content = content.replace("**", "").replace("*", "")
        content = content.replace("```", "").replace("`", "")

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        lines = [line.strip() for line in content.split("\n") if line.strip()]
        content = "\n".join(lines)

        return content

    def _extract_key_points(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–å…³é”®è¦ç‚¹"""
        points = []

        # æŒ‰å¥å­åˆ†å‰²
        sentences = content.split("ã€‚")

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 15 and len(sentence) < 120:  # åˆé€‚çš„é•¿åº¦
                # ç§»é™¤å¸¸è§çš„å¼€å¤´è¯
                clean_sentence = sentence
                for prefix in ["é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æ¥ç€", "æœ€å", "å¦å¤–", "æ­¤å¤–", "åŒæ—¶"]:
                    if clean_sentence.startswith(prefix):
                        clean_sentence = clean_sentence[len(prefix) :].strip()

                if clean_sentence and len(clean_sentence) > 10:
                    points.append(clean_sentence)

                # æœ€å¤šæå–3ä¸ªè¦ç‚¹
                if len(points) >= 3:
                    break

        return points

    def _assess_complexity(self, content: str) -> str:
        """è¯„ä¼°å¤æ‚åº¦"""
        content_lower = content.lower()

        # ç®€å•åˆ¤æ–­é€»è¾‘
        if any(word in content_lower for word in ["ç®€å•", "åŸºç¡€", "å®¹æ˜“", "ç›´æ¥"]):
            return "ğŸŸ¢ **å¤æ‚åº¦**: low"
        elif any(word in content_lower for word in ["å¤æ‚", "å›°éš¾", "é«˜çº§", "ä¸“ä¸š"]):
            return "ğŸ”´ **å¤æ‚åº¦**: high"
        else:
            return "ğŸŸ¡ **å¤æ‚åº¦**: medium"

    def _extract_suggested_tools(self, content: str) -> List[str]:
        """ä»æ€è€ƒå†…å®¹ä¸­æå–å»ºè®®çš„å·¥å…·ï¼ŒåŸºäºå®é™…å¯ç”¨çš„MCPå·¥å…·"""
        suggested_tools = []
        content_lower = content.lower()

        if not self.mcp_tools:
            return suggested_tools

        # éå†å¯ç”¨çš„MCPå·¥å…·ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ€è€ƒå†…å®¹ä¸­è¢«æåŠ
        for tool in self.mcp_tools:
            tool_name = getattr(tool, "name", str(tool)).lower()
            tool_desc = getattr(tool, "description", "").lower()

            # æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦åœ¨å†…å®¹ä¸­è¢«æåŠ
            if tool_name in content_lower:
                suggested_tools.append(tool_name)
                continue

            # æ£€æŸ¥å·¥å…·æè¿°ä¸­çš„å…³é”®è¯æ˜¯å¦åœ¨å†…å®¹ä¸­è¢«æåŠ
            # æå–æè¿°ä¸­çš„å…³é”®è¯ï¼ˆé€šå¸¸æ˜¯åŠŸèƒ½æè¿°ï¼‰
            desc_keywords = self._extract_tool_keywords(tool_desc)
            for keyword in desc_keywords:
                if keyword in content_lower:
                    suggested_tools.append(tool_name)
                    break

        # å»é‡å¹¶è¿”å›
        return list(set(suggested_tools))

    def _extract_tool_keywords(self, tool_desc: str) -> List[str]:
        """ä»å·¥å…·æè¿°ä¸­æå–å…³é”®è¯"""
        keywords = []

        # å¸¸è§çš„åŠŸèƒ½å…³é”®è¯
        common_keywords = [
            "æœç´¢",
            "æŸ¥è¯¢",
            "æŸ¥æ‰¾",
            "è·å–",
            "è¯»å–",
            "å†™å…¥",
            "ä¿å­˜",
            "åˆ†æ",
            "è®¡ç®—",
            "ç¿»è¯‘",
            "æœç´¢",
            "æŸ¥è¯¢",
            "æŸ¥æ‰¾",
            "è·å–",
            "è¯»å–",
            "å†™å…¥",
            "ä¿å­˜",
            "åˆ†æ",
            "è®¡ç®—",
            "ç¿»è¯‘",
            "search",
            "query",
            "find",
            "get",
            "read",
            "write",
            "save",
            "analyze",
            "calculate",
            "translate",
            "å­¦æœ¯",
            "æ–‡çŒ®",
            "è®ºæ–‡",
            "ä¸“åˆ©",
            "ç½‘ç»œ",
            "æ–‡ä»¶",
            "ä»£ç ",
            "å›¾åƒ",
            "å›¾ç‰‡",
            "academic",
            "literature",
            "paper",
            "patent",
            "web",
            "file",
            "code",
            "image",
            "picture",
        ]

        desc_lower = tool_desc.lower()
        for keyword in common_keywords:
            if keyword in desc_lower:
                keywords.append(keyword)

        return keywords

    def _build_thinking_input(self, user_input: str, context: Dict[str, Any] = None, conversation_history: List[Dict] = None) -> str:
        """æ„å»ºå®Œæ•´çš„æ€è€ƒè¾“å…¥"""

        input_parts = [self.thinking_prompt]

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context:
            input_parts.append(f"\nå½“å‰ä¸Šä¸‹æ–‡ï¼š\n{json.dumps(context, ensure_ascii=False, indent=2)}")

        # æ·»åŠ å¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼‰
        if conversation_history:
            recent_history = conversation_history[-5:]  # åªå–æœ€è¿‘5è½®
            history_text = "\n".join(
                [
                    f"{msg['role']}: {msg['content'][:200]}..." if len(msg["content"]) > 200 else f"{msg['role']}: {msg['content']}"
                    for msg in recent_history
                ]
            )
            input_parts.append(f"\nå¯¹è¯å†å²ï¼š\n{history_text}")

        # æ·»åŠ ç”¨æˆ·è¾“å…¥
        input_parts.append(f"\nç”¨æˆ·è¾“å…¥ï¼š\n{user_input}")

        input_parts.append("\nè¯·å¼€å§‹ä½ çš„æ·±åº¦æ€è€ƒåˆ†æï¼š")

        return "\n".join(input_parts)

    def _parse_thinking_response(self, response: str) -> ThinkingResult:
        """è§£æLLMçš„æ€è€ƒå“åº”"""
        try:
            # æ–°çš„è§£ææ–¹æ³•ï¼šä»è‡ªç„¶è¯­è¨€æ ¼å¼ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
            result = self._parse_natural_thinking_response(response)
            return result

        except Exception as e:
            logger.warning(f"è§£ææ€è€ƒç»“æœå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸå§‹å“åº”")
            return self._parse_fallback_response(response)

    def _parse_natural_thinking_response(self, response: str) -> ThinkingResult:
        """è§£æè‡ªç„¶è¯­è¨€æ ¼å¼çš„æ€è€ƒå“åº”"""

        # æå–ç”¨æˆ·æ„å›¾
        user_intent = self._extract_section_content(response, "ç”¨æˆ·æ„å›¾åˆ†æ")

        # æå–é—®é¢˜åˆ†æ
        problem_analysis = self._extract_section_content(response, "é—®é¢˜èƒŒæ™¯ç†è§£")

        # æå–æ‰§è¡Œè®¡åˆ’
        execution_plan_text = self._extract_section_content(response, "æ‰§è¡Œè®¡åˆ’åˆ¶å®š")
        execution_plan = self._parse_execution_plan(execution_plan_text)

        # æå–å¤æ‚åº¦è¯„ä¼°
        complexity_text = self._extract_section_content(response, "å¤æ‚åº¦è¯„ä¼°")
        estimated_complexity = self._extract_complexity(complexity_text)

        # æå–å»ºè®®æ¨¡å‹
        suggested_model = self._extract_section_content(response, "å»ºè®®æ¨¡å‹")

        # æ„å»ºThinkingResultå¯¹è±¡
        result = ThinkingResult(
            user_intent=user_intent or "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
            problem_analysis=problem_analysis or "é—®é¢˜åˆ†æä¸­",
            execution_plan=execution_plan,
            estimated_complexity=estimated_complexity,
            suggested_model=suggested_model,
            context_requirements={},
            timestamp=datetime.now(),
        )

        return result

    def _extract_section_content(self, text: str, section_name: str) -> str:
        """æå–æŒ‡å®šç« èŠ‚çš„å†…å®¹"""
        try:
            # æŸ¥æ‰¾ç« èŠ‚å¼€å§‹
            start_marker = f"**{section_name}**"
            start_idx = text.find(start_marker)
            if start_idx == -1:
                return ""

            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªç« èŠ‚æˆ–æ–‡æ¡£ç»“æŸ
            content_start = start_idx + len(start_marker)
            next_section_start = text.find("**", content_start)

            if next_section_start == -1:
                # æ²¡æœ‰ä¸‹ä¸€ä¸ªç« èŠ‚ï¼Œå–åˆ°æ–‡æ¡£ç»“æŸ
                content = text[content_start:].strip()
            else:
                # å–åˆ°ä¸‹ä¸€ä¸ªç« èŠ‚å¼€å§‹
                content = text[content_start:next_section_start].strip()

            # åªåšåŸºæœ¬çš„æ¸…ç†ï¼Œä¿ç•™AIçš„è‡ªç„¶è¯­è¨€è¾“å‡º
            content = content.strip()

            # ç§»é™¤å¯èƒ½çš„å ä½ç¬¦æ ‡è®°ï¼Œä½†ä¿ç•™å®é™…å†…å®¹
            if content.startswith("[") and content.endswith("]"):
                # å¦‚æœæ•´ä¸ªå†…å®¹è¢«æ–¹æ‹¬å·åŒ…å›´ï¼Œå¯èƒ½æ˜¯å ä½ç¬¦
                inner_content = content[1:-1].strip()
                if len(inner_content) > 10:  # å¦‚æœå†…å®¹è¶³å¤Ÿé•¿ï¼Œè®¤ä¸ºæ˜¯å®é™…å†…å®¹
                    content = inner_content
            elif content.startswith("[") and not content.endswith("]"):
                # å¦‚æœåªæœ‰å¼€å§‹æ–¹æ‹¬å·ï¼Œç§»é™¤å®ƒ
                content = content[1:].strip()

            return content.strip()

        except Exception as e:
            logger.debug(f"æå–ç« èŠ‚ {section_name} å¤±è´¥: {str(e)}")
            return ""

    def _parse_execution_plan(self, plan_text: str) -> List[ThinkingStep]:
        """è§£ææ‰§è¡Œè®¡åˆ’æ–‡æœ¬ - å¢å¼ºç‰ˆæœ¬"""
        steps = []
        if not plan_text:
            logger.debug("æ‰§è¡Œè®¡åˆ’æ–‡æœ¬ä¸ºç©º")
            return steps

        try:
            logger.debug(f"å¼€å§‹è§£ææ‰§è¡Œè®¡åˆ’: {plan_text[:200]}...")

            # æŒ‰è¡Œåˆ†å‰²
            lines = plan_text.split("\n")
            current_step = None

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                logger.debug(f"å¤„ç†è¡Œ: {line}")

                # å¤šç§æ­¥éª¤è¯†åˆ«æ¨¡å¼
                step_created = False

                # æ¨¡å¼1: æ•°å­—. æè¿°
                if line[0].isdigit() and "." in line:
                    step_created = self._create_step_from_line(line, steps, current_step)
                    if step_created:
                        current_step = step_created

                # æ¨¡å¼2: æ•°å­—ã€æè¿°
                elif line[0].isdigit() and "ã€" in line:
                    step_created = self._create_step_from_line(line.replace("ã€", "."), steps, current_step)
                    if step_created:
                        current_step = step_created

                # æ¨¡å¼3: - æè¿°
                elif line.startswith("- "):
                    step_created = self._create_step_from_line(f"{len(steps)+1}. {line[2:]}", steps, current_step)
                    if step_created:
                        current_step = step_created

                # æ¨¡å¼4: â€¢ æè¿°
                elif line.startswith("â€¢ "):
                    step_created = self._create_step_from_line(f"{len(steps)+1}. {line[2:]}", steps, current_step)
                    if step_created:
                        current_step = step_created

                # å¦‚æœä¸æ˜¯æ–°æ­¥éª¤ï¼Œæ·»åŠ åˆ°å½“å‰æ­¥éª¤çš„æè¿°ä¸­
                elif current_step and line and not step_created:
                    current_step.description += f" {line}"

            # æ·»åŠ æœ€åä¸€ä¸ªæ­¥éª¤
            if current_step:
                steps.append(current_step)

        except Exception as e:
            logger.debug(f"è§£ææ‰§è¡Œè®¡åˆ’å¤±è´¥: {str(e)}")

        # å¦‚æœæ²¡æœ‰è§£æåˆ°æ­¥éª¤ï¼Œå°è¯•ä»æ•´ä¸ªæ–‡æœ¬ä¸­æå–
        if not steps:
            steps = self._extract_steps_from_text(plan_text)

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ­¥éª¤ï¼Œåˆ›å»ºé»˜è®¤æ­¥éª¤
        if not steps:
            logger.debug("æ— æ³•è§£ææ‰§è¡Œè®¡åˆ’ï¼Œåˆ›å»ºé»˜è®¤æ­¥éª¤")
            steps = [
                ThinkingStep(
                    step_id="step_1",
                    description="æ ¹æ®ç”¨æˆ·è¾“å…¥ç›´æ¥å¤„ç†",
                    reasoning="æ— æ³•è§£æè¯¦ç»†è®¡åˆ’ï¼Œç›´æ¥æ‰§è¡Œç”¨æˆ·è¯·æ±‚",
                    expected_tools=[],
                    parameters={},
                )
            ]

        logger.debug(f"è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(steps)} ä¸ªæ­¥éª¤")
        return steps

    def _create_step_from_line(self, line: str, steps: List[ThinkingStep], current_step: ThinkingStep = None) -> ThinkingStep:
        """ä»è¡Œåˆ›å»ºæ­¥éª¤"""
        if current_step:
            steps.append(current_step)

        # æå–æ­¥éª¤æè¿°
        if "ï¼š" in line:
            step_desc = line.split("ï¼š", 1)[-1]
        elif "." in line:
            step_desc = line.split(".", 1)[-1]
        else:
            step_desc = line

        step_desc = step_desc.strip()

        # å¦‚æœæè¿°ä¸ºç©ºï¼Œä½¿ç”¨æ•´è¡Œä½œä¸ºæè¿°
        if not step_desc:
            step_desc = line.strip()

        new_step = ThinkingStep(
            step_id=f"step_{len(steps)+1}",
            description=step_desc,
            reasoning="åŸºäºç”¨æˆ·éœ€æ±‚åˆ¶å®šçš„æ‰§è¡Œæ­¥éª¤",
            expected_tools=[],
            parameters={},
            priority=len(steps) + 1,
            dependencies=[],
        )

        logger.debug(f"åˆ›å»ºæ­¥éª¤: {step_desc}")
        return new_step

    def _extract_steps_from_text(self, text: str) -> List[ThinkingStep]:
        """ä»æ–‡æœ¬ä¸­æå–æ­¥éª¤ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        steps = []

        # å°è¯•æŒ‰å¥å­åˆ†å‰²
        sentences = text.split("ã€‚")
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if len(sentence) > 10:  # åªå¤„ç†æœ‰æ„ä¹‰çš„å¥å­
                steps.append(
                    ThinkingStep(
                        step_id=f"step_{i+1}",
                        description=sentence,
                        reasoning="ä»æ–‡æœ¬ä¸­æå–çš„æ‰§è¡Œæ­¥éª¤",
                        expected_tools=[],
                        parameters={},
                        priority=i + 1,
                        dependencies=[],
                    )
                )

        return steps[:3]  # æœ€å¤šè¿”å›3ä¸ªæ­¥éª¤

    def _extract_complexity(self, complexity_text: str) -> str:
        """æå–å¤æ‚åº¦è¯„ä¼°"""
        if not complexity_text:
            return "medium"

        complexity_text = complexity_text.lower()

        if any(word in complexity_text for word in ["ç®€å•", "low", "easy"]):
            return "low"
        elif any(word in complexity_text for word in ["å¤æ‚", "high", "difficult"]):
            return "high"
        else:
            return "medium"

    def _parse_fallback_response(self, response: str) -> ThinkingResult:
        """å¤‡ç”¨è§£ææ–¹æ³•ï¼Œå½“JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨"""
        # åŸºäºå“åº”å†…å®¹åˆ›å»ºç®€å•çš„æ‰§è¡Œè®¡åˆ’
        steps = [
            ThinkingStep(
                step_id="step_1", description="æ ¹æ®ç”¨æˆ·è¾“å…¥ç›´æ¥å¤„ç†", reasoning="æ— æ³•è§£æè¯¦ç»†è®¡åˆ’ï¼Œç›´æ¥æ‰§è¡Œç”¨æˆ·è¯·æ±‚", expected_tools=[], parameters={}
            )
        ]

        return ThinkingResult(
            user_intent="éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
            problem_analysis=response[:500],  # å–å‰500å­—ç¬¦ä½œä¸ºåˆ†æ
            execution_plan=steps,
            estimated_complexity="medium",
            timestamp=datetime.now(),
        )

    def _create_fallback_result(self, user_input: str) -> ThinkingResult:
        """åˆ›å»ºå¤‡ç”¨çš„æ€è€ƒç»“æœ"""
        steps = [
            ThinkingStep(
                step_id="step_1",
                description=f"å¤„ç†ç”¨æˆ·è¯·æ±‚: {user_input[:100]}",
                reasoning="æ€è€ƒAgentå‡ºé”™ï¼Œç›´æ¥å¤„ç†ç”¨æˆ·è¾“å…¥",
                expected_tools=[],
                parameters={},
            )
        ]

        return ThinkingResult(
            user_intent=user_input[:200],
            problem_analysis="æ€è€ƒAgentæš‚æ—¶æ— æ³•åˆ†æï¼Œå°†ç›´æ¥å¤„ç†ç”¨æˆ·è¯·æ±‚",
            execution_plan=steps,
            estimated_complexity="medium",
            timestamp=datetime.now(),
        )

    async def refine_plan(self, current_plan: ThinkingResult, feedback: str) -> ThinkingResult:
        """
        æ ¹æ®åé¦ˆä¼˜åŒ–æ‰§è¡Œè®¡åˆ’

        Args:
            current_plan: å½“å‰æ‰§è¡Œè®¡åˆ’
            feedback: æ‰§è¡Œåé¦ˆæˆ–ç”¨æˆ·åé¦ˆ

        Returns:
            ThinkingResult: ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’
        """
        try:
            refine_prompt = f"""
åŸºäºä»¥ä¸‹æ‰§è¡Œè®¡åˆ’å’Œåé¦ˆï¼Œè¯·ä¼˜åŒ–å’Œè°ƒæ•´è®¡åˆ’ï¼š

å½“å‰è®¡åˆ’ï¼š
{json.dumps(current_plan.__dict__, ensure_ascii=False, default=str, indent=2)}

åé¦ˆä¿¡æ¯ï¼š
{feedback}

è¯·æä¾›ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’ï¼Œæ ¼å¼ä¸ä¹‹å‰ç›¸åŒçš„JSONæ ¼å¼ã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. æ ¹æ®åé¦ˆè°ƒæ•´æ­¥éª¤
2. ä¼˜åŒ–å·¥å…·é€‰æ‹©
3. æ”¹è¿›å‚æ•°é…ç½®
4. è°ƒæ•´æ­¥éª¤é¡ºåº

ä¼˜åŒ–åçš„è®¡åˆ’ï¼š
"""

            response = await self.llm.ainvoke(refine_prompt)
            refined_result = self._parse_thinking_response(response.content)

            logger.info("ThinkingAgentå·²æ ¹æ®åé¦ˆä¼˜åŒ–æ‰§è¡Œè®¡åˆ’")
            return refined_result

        except Exception as e:
            logger.error(f"è®¡åˆ’ä¼˜åŒ–å¤±è´¥: {str(e)}")
            return current_plan  # è¿”å›åŸè®¡åˆ’

    def get_thinking_stats(self) -> Dict[str, Any]:
        """è·å–æ€è€ƒAgentçš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "provider": self.provider,
            "model": self.model_name,
            "capabilities": ["intent_analysis", "problem_decomposition", "execution_planning", "complexity_assessment", "plan_optimization"],
            "config": self.llm_kwargs,
        }

    def _clean_thinking_chunk(self, chunk: str) -> str:
        """æ¸…ç†æ€è€ƒåˆ†å—å†…å®¹ï¼Œç§»é™¤markdownæ ‡è®°ç­‰"""
        if not chunk:
            return ""

        # ç§»é™¤markdownæ ‡è®°
        chunk = chunk.replace("###", "").replace("##", "").replace("#", "")
        chunk = chunk.replace("**", "").replace("*", "")
        chunk = chunk.replace("```", "").replace("`", "")

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        lines = [line.strip() for line in chunk.split("\n") if line.strip()]
        chunk = "\n".join(lines)

        return chunk.strip()

    def _create_structured_result(self, full_response: str, user_input: str) -> str:
        """åˆ›å»ºç»“æ„åŒ–çš„æ€è€ƒç»“æœ - JSONæ ¼å¼"""

        # æ¸…ç†å’Œæ ¼å¼åŒ–æ€è€ƒå†…å®¹
        cleaned_response = self._clean_summary_content(full_response)

        # æå–å…³é”®ä¿¡æ¯
        key_points = self._extract_key_points(cleaned_response)
        complexity = self._assess_complexity(cleaned_response)

        # æå–å»ºè®®çš„å·¥å…·
        suggested_tools = self._extract_suggested_tools(cleaned_response)

        # æ„å»ºç»“æ„åŒ–æ•°æ®
        structured_data = {
            "status": "completed",
            "user_input": user_input,
            "user_intent": user_input[:200] + "..." if len(user_input) > 200 else user_input,
            "problem_analysis": cleaned_response[:800] + "..." if len(cleaned_response) > 800 else cleaned_response,
            "key_points": key_points,
            "execution_plan": [
                {
                    "step_id": "step_1",
                    "description": "åŸºäºæ€è€ƒç»“æœæ‰§è¡Œç”¨æˆ·è¯·æ±‚",
                    "reasoning": "æ ¹æ®AIçš„æ€è€ƒåˆ†ææ‰§è¡Œä»»åŠ¡",
                    "expected_tools": suggested_tools,  # ä½¿ç”¨æå–çš„å·¥å…·ä¿¡æ¯
                    "parameters": {},
                    "priority": 1,
                    "dependencies": [],
                }
            ],
            "estimated_complexity": complexity.replace("ğŸŸ¢ **å¤æ‚åº¦**: ", "").replace("ğŸŸ¡ **å¤æ‚åº¦**: ", "").replace("ğŸ”´ **å¤æ‚åº¦**: ", ""),
            "complexity_level": "low" if "ğŸŸ¢" in complexity else "high" if "ğŸ”´" in complexity else "medium",
            "suggested_model": None,
            "context_requirements": {},
            "suggested_tools": suggested_tools,  # æ·»åŠ å»ºè®®å·¥å…·å­—æ®µ
            "thinking_duration": "completed",
            "timestamp": datetime.now().isoformat(),
            "metadata": {
                "response_length": len(full_response),
                "key_points_count": len(key_points),
                "analysis_quality": "good" if len(key_points) >= 2 else "basic",
                "tools_suggested": len(suggested_tools),
            },
        }

        return json.dumps(structured_data, ensure_ascii=False, indent=2)
